import pandas as pd
import numpy as np
from thefuzz import fuzz
import nltk
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load the Excel Files
df1 = pd.read_excel(r'ControlsEngine\one.xlsx')
df2 = pd.read_excel(r'ControlsEngine\otwo.xlsx')

# Forward-fill missing 'Domain' and 'Sub-Domain' values due to merged cells
df1['Domain'] = df1['Domain'].fillna(method='ffill')
df2['Domain'] = df2['Domain'].fillna(method='ffill')
df1['Sub-Domain'] = df1['Sub-Domain'].fillna(method='ffill')
df2['Sub-Domain'] = df2['Sub-Domain'].fillna(method='ffill')

# Download necessary NLTK data files (only needed once)
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Initialize Lemmatizer and Stop Words
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Text Preprocessing Function
def preprocess_text(text):
    if pd.isnull(text):
        return ''
    text = str(text).lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    words = text.split()
    # Commented out stop words removal to retain important words
    # words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(words)

# Combine and Preprocess Text Columns
# Combine 'Domain', 'Sub-Domain', and 'Control' for context
df1['Combined_Text'] = df1['Domain'].astype(str) + ' ' + df1['Sub-Domain'].astype(str) + ' ' + df1['Control'].astype(str)
df2['Combined_Text'] = df2['Domain'].astype(str) + ' ' + df2['Sub-Domain'].astype(str) + ' ' + df2['Control'].astype(str)

df1['Processed_Control'] = df1['Combined_Text'].apply(preprocess_text)
df2['Processed_Control'] = df2['Combined_Text'].apply(preprocess_text)

# Initialize Sentence Transformer with a better model
model = SentenceTransformer('all-mpnet-base-v2')

# Match Domains Using Semantic Similarity
domains1 = df1['Domain'].dropna().unique().tolist()
domains2 = df2['Domain'].dropna().unique().tolist()

# Define manual domain mappings
manual_domain_mapping = {
    'Security Incident Management': 'Resilience',
    'Application & Software': 'Secure Software Development Lifecycle',
    # Add more mappings as needed
}

# Define domain descriptions for better context
domain_descriptions = {
    'Security Incident Management': 'Management of security incidents and response',
    'Resilience': 'Organizational resilience and disaster recovery',
    'Application & Software': 'Development and management of applications and software',
    'Secure Software Development Lifecycle': 'Practices for secure software development',
    # Add more descriptions as needed
}

# Preprocess domain names with enhanced descriptions
domain_texts1 = [preprocess_text(domain_descriptions.get(d, d)) for d in domains1]
domain_texts2 = [preprocess_text(domain_descriptions.get(d, d)) for d in domains2]

# Encode domains
domain_embeddings1 = model.encode(domain_texts1, convert_to_tensor=True)
domain_embeddings2 = model.encode(domain_texts2, convert_to_tensor=True)

# Compute semantic similarity matrix
domain_similarity_matrix = util.cos_sim(domain_embeddings1, domain_embeddings2).cpu().numpy()

# Compute fuzzy string similarity matrix
fuzzy_scores = []
for d1 in domains1:
    scores = [fuzz.token_set_ratio(d1, d2) for d2 in domains2]
    fuzzy_scores.append(scores)
fuzzy_similarity_matrix = np.array(fuzzy_scores) / 100  # Normalize to 0-1

# Combine similarities
combined_similarity_matrix = (domain_similarity_matrix * 0.6 + fuzzy_similarity_matrix * 0.4)

# Threshold for domain matching
domain_similarity_threshold = 0.44  # Adjusted threshold

# Match domains based on combined similarity and manual mappings
domain_mapping = {}
for idx1, d1 in enumerate(domains1):
    if d1 in manual_domain_mapping:
        domain_mapping[d1] = manual_domain_mapping[d1]
        print(f"Manual mapping: {d1} --> {domain_mapping[d1]}")
    else:
        similarities = combined_similarity_matrix[idx1]
        best_idx = similarities.argmax()
        best_score = similarities[best_idx]
        if best_score >= domain_similarity_threshold:
            d2 = domains2[best_idx]
            domain_mapping[d1] = d2
            print(f"Matched {d1} --> {d2} (Similarity: {best_score:.2f})")
        else:
            print(f"No match for Domain: {d1} (Best score: {best_score:.2f})")

# Print matched domains for debugging
print("\nFinal Matched Domains:")
for d1, d2 in domain_mapping.items():
    print(f"{d1} --> {d2}")

# Match Controls using Combined Similarity with Weights
results = []
tfidf_weight = 0.4
embedding_weight = 0.6  # Adjusted weight to emphasize embeddings
control_threshold = 0.35  # Adjusted threshold
top_k = 6  # Limit to top 3 matches per control

# Loop Through Matched Domains
for domain_f1, domain_f2 in domain_mapping.items():
    controls_f1 = df1[df1['Domain'] == domain_f1].reset_index(drop=True)
    controls_f2 = df2[df2['Domain'] == domain_f2].reset_index(drop=True)
    
    if controls_f1.empty or controls_f2.empty:
        continue

    texts_f1 = controls_f1['Processed_Control'].tolist()
    texts_f2 = controls_f2['Processed_Control'].tolist()

    # Embedding Similarity
    embeddings_f1 = model.encode(texts_f1, convert_to_tensor=True)
    embeddings_f2 = model.encode(texts_f2, convert_to_tensor=True)
    embedding_similarity = util.cos_sim(embeddings_f1, embeddings_f2).cpu().numpy()

    # TF-IDF Similarity
    vectorizer = TfidfVectorizer().fit(texts_f1 + texts_f2)
    tfidf_f1 = vectorizer.transform(texts_f1)
    tfidf_f2 = vectorizer.transform(texts_f2)
    tfidf_similarity = cosine_similarity(tfidf_f1, tfidf_f2)

    # Combined Similarity
    combined_similarity = (embedding_similarity * embedding_weight +
                           tfidf_similarity * tfidf_weight) / (embedding_weight + tfidf_weight)

    for idx_f1, row_f1 in controls_f1.iterrows():
        similarities = combined_similarity[idx_f1]
        # Get indices of top_k highest similarities
        top_indices = similarities.argsort()[-top_k:][::-1]
        top_scores = similarities[top_indices]
        # Filter matches based on threshold
        matching_indices = [i for i, score in zip(top_indices, top_scores) if score >= control_threshold]
        matching_controls = [controls_f2.iloc[i]['Control'] for i in matching_indices]
        matching_scores = [similarities[i] for i in matching_indices]
        
        if not matching_controls:
            matching_controls = [None]
            matching_scores = [None]
        
        results.append({
            'Domain': domain_f1,
            'Sub-Domain': row_f1['Sub-Domain'],
            'Control': row_f1['Control'],
            'Controls from F2 that match': matching_controls,
            'Similarity Scores': matching_scores
        })

# Brute Force Matching for Unmatched Domains
# Extract unmatched controls
unmatched_domains_f1 = set(domains1) - set(domain_mapping.keys())
unmatched_domains_f2 = set(domains2) - set(domain_mapping.values())

unmatched_controls_f1 = df1[df1['Domain'].isin(unmatched_domains_f1)].reset_index(drop=True)
unmatched_controls_f2 = df2[df2['Domain'].isin(unmatched_domains_f2)].reset_index(drop=True)

texts_f1 = unmatched_controls_f1['Processed_Control'].tolist()
texts_f2 = unmatched_controls_f2['Processed_Control'].tolist()

if texts_f1 and texts_f2:
    # Embedding Similarity
    embeddings_f1 = model.encode(texts_f1, convert_to_tensor=True)
    embeddings_f2 = model.encode(texts_f2, convert_to_tensor=True)
    embedding_similarity = util.cos_sim(embeddings_f1, embeddings_f2).cpu().numpy()

    # TF-IDF Similarity
    vectorizer = TfidfVectorizer().fit(texts_f1 + texts_f2)
    tfidf_f1 = vectorizer.transform(texts_f1)
    tfidf_f2 = vectorizer.transform(texts_f2)
    tfidf_similarity = cosine_similarity(tfidf_f1, tfidf_f2)

    # Combined Similarity
    combined_similarity = (embedding_similarity * embedding_weight +
                           tfidf_similarity * tfidf_weight) / (embedding_weight + tfidf_weight)

    for idx_f1, row_f1 in unmatched_controls_f1.iterrows():
        similarities = combined_similarity[idx_f1]
        # Get indices of top_k highest similarities
        top_indices = similarities.argsort()[-top_k:][::-1]
        top_scores = similarities[top_indices]
        # Filter matches based on threshold
        matching_indices = [i for i, score in zip(top_indices, top_scores) if score >= control_threshold]
        matching_controls = [unmatched_controls_f2.iloc[i]['Control'] for i in matching_indices]
        matching_scores = [similarities[i] for i in matching_indices]
        
        if not matching_controls:
            matching_controls = [None]
            matching_scores = [None]
        
        results.append({
            'Domain': row_f1['Domain'],
            'Sub-Domain': row_f1['Sub-Domain'],
            'Control': row_f1['Control'],
            'Controls from F2 that match': matching_controls,
            'Similarity Scores': matching_scores
        })

# Merge Results and Format Output
final_df = pd.DataFrame(results)
final_df = final_df.explode(['Controls from F2 that match', 'Similarity Scores'])
final_df = final_df.reset_index(drop=True)
final_df['Similarity Scores'] = final_df['Similarity Scores'].apply(lambda x: f"{x:.2f}" if pd.notnull(x) else '')

# Save to Excel
final_df.to_excel('output.xlsx', index=False)
